{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Unit 1: Model Benchmark Challenge\n"
      ],
      "metadata": {
        "id": "wFXKFXQAnHDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet\n"
      ],
      "metadata": {
        "id": "YewFDqWEnIVw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "8L5IdCwWnZmL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Text Generation\n",
        "### Model: BERT (bert-base-uncased)\n"
      ],
      "metadata": {
        "id": "jOjGuUFLntlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "bert_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "bert_generator(\"The future of Artificial Intelligence is\", max_length=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfQTMpLTnu3E",
        "outputId": "244bfc78-dea0-4e1e-ba7c-7d891bcdd5de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is................................................................................................................................................................................................................................................................'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: RoBERTa (roberta-base)\n"
      ],
      "metadata": {
        "id": "dwDe6VFYoFLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "roberta_generator(\"The future of Artificial Intelligence is\", max_length=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJksCkYDoGEQ",
        "outputId": "eab0e934-0102-4839-c200-18df53b897c5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence is'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: BART (facebook/bart-base)\n"
      ],
      "metadata": {
        "id": "2tQnnbD6oTgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bart_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "bart_generator(\"The future of Artificial Intelligence is\", max_length=40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7_ugTJ4oUsg",
        "outputId": "421de368-c0ec-4343-b7ec-97660634fe7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'The future of Artificial Intelligence istips competenttipstipsScore Un Veteranossibilitytipstipstips Surfacetipsbuf MediumtipsIncreases crawl crawl DOIIncreases DOIbonestipsIncreases DOI Vanilla Jeff Jeff perkScore faultyScore toughnessUnit neither Jeff flubuf faulty faultytips faulty faulty Wh Lay Koraching faultyfull faulty faulty faultybuf 291.[ Papers faulty competenttipsanalysisGround Papers Iristips faulty Lay faulty faulty Layivalry faulty faulty Kor competent parked brainstipstips faulty Sod Kortips Kor PapersSimple faultyruction Laytips Laytips Papersoak Kor Paperstips faulty324324tips Commonwealth faulty faultyivalry Kor parkedtipstips barbaric Kortips cautious Kor Laytips Noisetips faulty Marriage 247tips Lay bittentipstips psychiatric faulty faulty Papers Kor Kor concedtips oversized Layfull Laytips faulty competent Kor faulty Papers mortals faulty decisive faulty Laytips Commonwealth Lay faulty Kor faulty Teachers faulty fract faulty faulty Commonwealth Kor Marriage fractPutin Lay Laypicking Teacherstips tending faulty Marriage Kor Marriage faulty faulty tending Kortips bitten Marriagetips324 Laytips Marriagetips competentrogen dropped LayPutin Laytips playful Paperstips KorarynPutinPutinPutin Lay 291 playful Lay Lay Commonwealth Laytips brains Kor bittentips faulty bitten Commonwealth Commonwealth bitten faultytips Muroak Lay LayPutintips Lay resideoak faulty faulty floors Lay parked Lay Lay Laytips Kor Marriage bitten Marriage Surfacetips Commonwealth Commonwealth Lay Commonwealthtipstipsaryn Laytips'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Masked Language Modeling (Fill-Mask)\n",
        "### Model: BERT (bert-base-uncased)\n"
      ],
      "metadata": {
        "id": "J3xgGEQfovNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "bert_mask(\"The goal of Generative AI is to [MASK] new content.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKpsTFLhowN5",
        "outputId": "a1fc2a38-11a3-4711-9343-b955de130f03"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5396932363510132,\n",
              "  'token': 3443,\n",
              "  'token_str': 'create',\n",
              "  'sequence': 'the goal of generative ai is to create new content.'},\n",
              " {'score': 0.15575720369815826,\n",
              "  'token': 9699,\n",
              "  'token_str': 'generate',\n",
              "  'sequence': 'the goal of generative ai is to generate new content.'},\n",
              " {'score': 0.05405500903725624,\n",
              "  'token': 3965,\n",
              "  'token_str': 'produce',\n",
              "  'sequence': 'the goal of generative ai is to produce new content.'},\n",
              " {'score': 0.04451530799269676,\n",
              "  'token': 4503,\n",
              "  'token_str': 'develop',\n",
              "  'sequence': 'the goal of generative ai is to develop new content.'},\n",
              " {'score': 0.01757744885981083,\n",
              "  'token': 5587,\n",
              "  'token_str': 'add',\n",
              "  'sequence': 'the goal of generative ai is to add new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: RoBERTa (roberta-base)\n"
      ],
      "metadata": {
        "id": "FwaIGKQGo-ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "roberta_mask(\"The goal of Generative AI is to <mask> new content.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck2Zt_n6o_cN",
        "outputId": "52950854-bc7d-4df1-adba-839bbf740d1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3711312413215637,\n",
              "  'token': 5368,\n",
              "  'token_str': ' generate',\n",
              "  'sequence': 'The goal of Generative AI is to generate new content.'},\n",
              " {'score': 0.3677145540714264,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.08351420611143112,\n",
              "  'token': 8286,\n",
              "  'token_str': ' discover',\n",
              "  'sequence': 'The goal of Generative AI is to discover new content.'},\n",
              " {'score': 0.021335121244192123,\n",
              "  'token': 465,\n",
              "  'token_str': ' find',\n",
              "  'sequence': 'The goal of Generative AI is to find new content.'},\n",
              " {'score': 0.016521666198968887,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: BART (facebook/bart-base)\n"
      ],
      "metadata": {
        "id": "isYFAZXKpLBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bart_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "bart_mask(\"The goal of Generative AI is to <mask> new content.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1RKBSk8pMSw",
        "outputId": "a1292992-dbc8-4a50-90b8-4cdb20da475f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07461541891098022,\n",
              "  'token': 1045,\n",
              "  'token_str': ' create',\n",
              "  'sequence': 'The goal of Generative AI is to create new content.'},\n",
              " {'score': 0.06571870297193527,\n",
              "  'token': 244,\n",
              "  'token_str': ' help',\n",
              "  'sequence': 'The goal of Generative AI is to help new content.'},\n",
              " {'score': 0.060880109667778015,\n",
              "  'token': 694,\n",
              "  'token_str': ' provide',\n",
              "  'sequence': 'The goal of Generative AI is to provide new content.'},\n",
              " {'score': 0.03593561053276062,\n",
              "  'token': 3155,\n",
              "  'token_str': ' enable',\n",
              "  'sequence': 'The goal of Generative AI is to enable new content.'},\n",
              " {'score': 0.03319477662444115,\n",
              "  'token': 1477,\n",
              "  'token_str': ' improve',\n",
              "  'sequence': 'The goal of Generative AI is to improve new content.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3: Question Answering\n",
        "### Model: BERT (bert-base-uncased)\n"
      ],
      "metadata": {
        "id": "hkiwXc1CpcmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "bert_qa(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn7tngxMpbzq",
        "outputId": "cf9d3908-2de1-4349-9e44-6594af05389d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.02191578270867467,\n",
              " 'start': 46,\n",
              " 'end': 82,\n",
              " 'answer': 'hallucinations, bias, and deepfakes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: RoBERTa (roberta-base)\n"
      ],
      "metadata": {
        "id": "Q7YtS2gbppmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "roberta_qa(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s562ttnlprG5",
        "outputId": "0205425a-a748-48a9-91ec-98e51d193273"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.005414959508925676,\n",
              " 'start': 38,\n",
              " 'end': 71,\n",
              " 'answer': 'such as hallucinations, bias, and'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model: BART (facebook/bart-base)\n"
      ],
      "metadata": {
        "id": "LcQJ1uX3p264"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bart_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"facebook/bart-base\"\n",
        ")\n",
        "\n",
        "bart_qa(\n",
        "    question=\"What are the risks?\",\n",
        "    context=\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1uoUJ__p3ly",
        "outputId": "51332125-b128-4025-8e75-ddd3c699cf88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.022303341887891293,\n",
              " 'start': 0,\n",
              " 'end': 31,\n",
              " 'answer': 'Generative AI poses significant'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observation Table\n",
        "\n",
        "| Task | Model | Classification (Success / Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|-----|------|------------------------------------|--------------------------------------|--------------------------------------------|\n",
        "| Generation | BERT | Failure | Returned only the input prompt without generating new text | BERT is an encoder only model trained with masked language modeling and lacks a decoder for autoregressive generation |\n",
        "| Generation | RoBERTa | Failure | Echoed the input prompt without producing any continuation | RoBERTa is also an encoder only architecture and cannot generate tokens sequentially |\n",
        "| Generation | BART | Success | Generated a continuation with new tokens, though the output was repetitive and noisy. | BART has an encoder decoder architecture with a decoder capable of autoregressive generation |\n",
        "| Fill Mask | BERT | Success | Correctly predicted words like “create” and “generate” with high confidence. | BERT is trained using masked language modeling (MLM) |\n",
        "| Fill-Mask | RoBERTa | Success | Accurately predicted context-aware words such as “generate” and “create” | RoBERTa is optimized for MLM with improved training strategies. |\n",
        "| Fill-Mask | BART | Partial Success | Produced reasonable predictions but with lower confidence | BART is trained as a denoising autoencoder, not pure MLM |\n",
        "| QA | BERT | Partial Success | Returned the correct answer but with very low confidence | The model is not fine-tuned for question answering tasks |\n",
        "| QA | RoBERTa | Partial Success | Returned only a partial answer span with low confidence | Encoder only model without QA specific fine tuning |\n",
        "| QA | BART | Partial Success | Returned a partial span of the answer with low confidence | Encoder decoder model not fine- uned for extractive QA |\n"
      ],
      "metadata": {
        "id": "eWQFwFlkqNSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TGpJu3RyqPJ5"
      }
    }
  ]
}